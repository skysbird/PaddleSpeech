# This configuration tested on 8 GPUs (A100) with 80GB GPU memory.
# It takes around 4 days to finish the training,You can adjust
# batch_size„ÄÅnum_workers here and ngpu in local/train.sh for your machine
###########################################################
#                FEATURE EXTRACTION SETTING               #
###########################################################

fs: 24000          # sr
n_fft: 2048        # FFT size (samples).
n_shift: 300       # Hop size (samples). 12.5ms
win_length: 1200   # Window length (samples). 50ms
                   # If set to null, it will be the same as fft_size.
window: "hann"     # Window function.

# Only used for feats_type != raw

fmin: 80           # Minimum frequency of Mel basis.
fmax: 7600         # Maximum frequency of Mel basis.
n_mels: 80         # The number of mel basis.

mean_phn_span: 8
mlm_prob: 0.8

###########################################################
#                       DATA SETTING                      #
###########################################################
batch_size: 10
num_workers: 8

###########################################################
#                       MODEL SETTING                     #
###########################################################
model:
    text_masking: true
    postnet_layers: 5
    postnet_filts: 5
    postnet_chans: 256
    encoder_type: conformer
    decoder_type: conformer
    enc_input_layer: sega_mlm
    enc_pre_speech_layer: 0
    enc_cnn_module_kernel: 7
    enc_attention_dim: 384
    enc_attention_heads: 2
    enc_linear_units: 1536
    enc_num_blocks: 4
    enc_dropout_rate: 0.2
    enc_positional_dropout_rate: 0.2
    enc_attention_dropout_rate: 0.2
    enc_normalize_before: true
    enc_macaron_style: true
    enc_use_cnn_module: true
    enc_selfattention_layer_type: legacy_rel_selfattn
    enc_activation_type: swish
    enc_pos_enc_layer_type: legacy_rel_pos
    enc_positionwise_layer_type: conv1d
    enc_positionwise_conv_kernel_size: 3
    dec_cnn_module_kernel: 31
    dec_attention_dim: 384
    dec_attention_heads: 2
    dec_linear_units: 1536
    dec_num_blocks: 4
    dec_dropout_rate: 0.2
    dec_positional_dropout_rate: 0.2
    dec_attention_dropout_rate: 0.2
    dec_macaron_style: true
    dec_use_cnn_module: true
    dec_selfattention_layer_type: legacy_rel_selfattn
    dec_activation_type: swish
    dec_pos_enc_layer_type: legacy_rel_pos
    dec_positionwise_layer_type: conv1d
    dec_positionwise_conv_kernel_size: 3

###########################################################
#                     OPTIMIZER SETTING                   #
###########################################################
scheduler_params:
    d_model: 384
    warmup_steps: 4000
grad_clip: 1.0

###########################################################
#                     TRAINING SETTING                    #
###########################################################
max_epoch: 5000000
num_snapshots: 50

###########################################################
#                       OTHER SETTING                     #
###########################################################
seed: 0

token_list:
- <pad>
- <unk>
- AA0
- AA1
- AA2
- AE0
- AE1
- AE2
- AH0
- AH1
- AH2
- AO0
- AO1
- AO2
- AW0
- AW1
- AW2
- AY0
- AY1
- AY2
- B
- CH
- D
- DH
- EH0
- EH1
- EH2
- ER0
- ER1
- ER2
- EY0
- EY1
- EY2
- F
- G
- HH
- IH0
- IH1
- IH2
- IY0
- IY1
- IY2
- JH
- K
- L
- M
- N
- NG
- OW0
- OW1
- OW2
- OY0
- OY1
- OY2
- P
- R
- S
- SH
- T
- TH
- UH0
- UH1
- UH2
- UW0
- UW1
- UW2
- V
- W
- Y
- Z
- ZH
- a1
- a2
- a3
- a4
- a5
- ai1
- ai2
- ai3
- ai4
- ai5
- air2
- air3
- air4
- an1
- an2
- an3
- an4
- an5
- ang1
- ang2
- ang3
- ang4
- ang5
- angr2
- angr4
- anr1
- anr3
- anr4
- ao1
- ao2
- ao3
- ao4
- ao5
- aor1
- aor3
- aor4
- aor5
- ar2
- ar3
- ar4
- ar5
- b
- c
- ch
- d
- e1
- e2
- e3
- e4
- e5
- ei1
- ei2
- ei3
- ei4
- ei5
- eir4
- en1
- en2
- en3
- en4
- en5
- eng1
- eng2
- eng3
- eng4
- eng5
- engr4
- enr1
- enr2
- enr3
- enr4
- enr5
- er1
- er2
- er3
- er4
- er5
- f
- g
- h
- i1
- i2
- i3
- i4
- i5
- ia1
- ia2
- ia3
- ia4
- ia5
- ian1
- ian2
- ian3
- ian4
- ian5
- iang1
- iang2
- iang3
- iang4
- iang5
- iangr4
- ianr1
- ianr2
- ianr3
- ianr4
- ianr5
- iao1
- iao2
- iao3
- iao4
- iao5
- iaor1
- iaor2
- iaor3
- iaor4
- iar1
- iar4
- ie1
- ie2
- ie3
- ie4
- ie5
- ii1
- ii2
- ii3
- ii4
- ii5
- iii1
- iii2
- iii3
- iii4
- iii5
- iiir1
- iiir4
- in1
- in2
- in3
- in4
- in5
- ing1
- ing2
- ing3
- ing4
- ing5
- ingr1
- ingr2
- ingr3
- ingr4
- inr1
- inr4
- io1
- io3
- iong1
- iong2
- iong3
- iong4
- iong5
- iou1
- iou2
- iou3
- iou4
- iou5
- iour1
- iour2
- iour3
- iour4
- ir1
- ir3
- ir4
- j
- k
- l
- m
- n
- o1
- o2
- o3
- o4
- o5
- ong1
- ong2
- ong3
- ong4
- ong5
- or2
- ou1
- ou2
- ou3
- ou4
- ou5
- our2
- our3
- our4
- our5
- p
- q
- r
- s
- sh
- sil
- sp
- spl
- spn
- t
- u1
- u2
- u3
- u4
- u5
- ua1
- ua2
- ua3
- ua4
- ua5
- uai1
- uai2
- uai3
- uai4
- uair4
- uan1
- uan2
- uan3
- uan4
- uan5
- uang1
- uang2
- uang3
- uang4
- uangr4
- uanr2
- uanr3
- uanr4
- uanr5
- uar1
- uar2
- uar4
- uei1
- uei2
- uei3
- uei4
- uei5
- ueir1
- ueir2
- ueir3
- ueir4
- uen1
- uen2
- uen3
- uen4
- uen5
- ueng1
- ueng4
- uenr1
- uenr2
- uenr3
- uenr4
- uo1
- uo2
- uo3
- uo4
- uo5
- uor1
- uor2
- uor3
- uor5
- ur1
- ur2
- ur4
- ur5
- v1
- v2
- v3
- v4
- v5
- van1
- van2
- van3
- van4
- van5
- vanr1
- vanr2
- vanr3
- vanr4
- ve1
- ve2
- ve3
- ve4
- ve5
- ver3
- ver4
- vn1
- vn2
- vn3
- vn4
- vn5
- vnr2
- vr3
- x
- z
- zh
- <eos>
